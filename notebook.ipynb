{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame Shape: (184843, 7)\n",
      "Columns: Index(['sender', 'receiver', 'date', 'subject', 'body', 'urls', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def list_csv_files(directory):\n",
    "    # List to hold csv file names\n",
    "    csv_files = []\n",
    "    \n",
    "    # Iterate over files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        # Check if the file is a CSV\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_files.append(filename)\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "folder_path = './phishing_data'\n",
    "csv_files = list_csv_files(folder_path)\n",
    "\n",
    "# List to hold dataframes\n",
    "combined = []\n",
    "\n",
    "# Specify the required columns\n",
    "required_columns = ['sender', 'receiver', 'date', 'subject', 'body', 'urls', 'label']\n",
    "\n",
    "# Process each file\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    \n",
    "    # Read with TREC-specific parameters if needed\n",
    "    if 'TREC' in file:\n",
    "        df = pd.read_csv(file_path, on_bad_lines=\"warn\", lineterminator=\"\\n\")\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Check if the dataset contains the required 7 columns\n",
    "    if set(required_columns).issubset(df.columns):\n",
    "        # Only select the required columns\n",
    "        df = df[required_columns]\n",
    "        combined.append(df)\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "combined_df = pd.concat(combined, ignore_index=True)\n",
    "\n",
    "# Print the combined dataframe details\n",
    "print(f\"Combined DataFrame Shape: {combined_df.shape}\")\n",
    "print(f\"Columns: {combined_df.columns}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame Shape after removing missing values: (174904, 7)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing values in any of the columns\n",
    "combined_df_cleaned = combined_df.dropna()\n",
    "\n",
    "# Print the shape of the cleaned DataFrame to confirm removal of missing values\n",
    "print(f\"Combined DataFrame Shape after removing missing values: {combined_df_cleaned.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to string except for 'date' and 'label'\n",
    "combined_df = combined_df.astype({\n",
    "    'sender': str,\n",
    "    'receiver': str,\n",
    "    'subject': str,\n",
    "    'body': str,\n",
    "    'urls': str,\n",
    "    'label': int\n",
    "})\n",
    "\n",
    "# Convert 'date' column to datetime, using `errors='coerce'` to handle bad date strings\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce', utc=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA, we came to realise that most of the URLs in the dataset was labelled 0 or 1. Hence, for the small percentage of rows that contain actual URL strings, we converted them to 1 and 0 too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to handle the URL column conversion only if necessary\n",
    "def convert_urls(url_column_value):\n",
    "    # If it's already 0 or 1, return as is\n",
    "    if url_column_value == 0 or url_column_value == 1:\n",
    "        return url_column_value\n",
    "    # If it's an empty list \"[]\", convert to 0\n",
    "    elif url_column_value == \"[]\":\n",
    "        return 0\n",
    "    # If it's a non-empty list (e.g., [http://example.com]), convert to 1\n",
    "    elif url_column_value.startswith(\"[\") and url_column_value.endswith(\"]\"):\n",
    "        return 1\n",
    "    return 0  # Default fallback if not recognized\n",
    "\n",
    "# Apply the conversion function to the 'urls' column\n",
    "combined_df['urls'] = combined_df['urls'].apply(convert_urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to string except for 'date' and 'label'\n",
    "combined_df = combined_df.astype({\n",
    "    'sender': str,\n",
    "    'receiver': str,\n",
    "    'subject': str,\n",
    "    'body': str,\n",
    "    'label': int\n",
    "})\n",
    "\n",
    "# Convert 'date' column to datetime, using `errors='coerce'` to handle bad date strings\n",
    "combined_df['date'] = pd.to_datetime(combined_df['date'], errors='coerce', utc=True)\n",
    "\n",
    "# Remove rows where 'date' column is NaT (invalid date)\n",
    "combined_df = combined_df.dropna(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the feature engineering for the date column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_business_hours(hour):\n",
    "    return 9 <= hour < 17\n",
    "\n",
    "def extract_date_features(df):\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    # Numerise the days of the week e.g. 0 = Monday, 6 = Sunday\n",
    "    features['date_day_of_week'] = df['date'].dt.dayofweek \n",
    "\n",
    "    # Numerise the time of the day by the hour in 24 hour format\n",
    "    features['date_hour_of_day'] = df['date'].dt.hour\n",
    "\n",
    "    # Checks whether it is the weekend\n",
    "    features['date_is_weekend'] = df['date'].dt.dayofweek >= 5\n",
    "\n",
    "    # Checks whether it is during business hour\n",
    "    features['date_is_business_hours'] = features['date_hour_of_day'].apply(is_business_hours)\n",
    "\n",
    "    return features\n",
    "\n",
    "phishing_date_features = extract_date_features(combined_df)\n",
    "combined_df = pd.concat([combined_df.reset_index(drop=True), phishing_date_features.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering for subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subject_features(df):\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    # Checks if the email is a reply to any previous mail\n",
    "    features['subj_reply'] = df['subject'].apply(lambda x: x.lower().startswith(\"re:\"))\n",
    "\n",
    "    # Checks if the email is a forward from another mail\n",
    "    features['subj_forward'] = df['subject'].apply(lambda x: x.lower().startswith(\"fwd:\"))\n",
    "\n",
    "    # Number of words in the subject\n",
    "    features['subj_word_count'] = df['subject'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Number of characters in the subject\n",
    "    features['subj_char_count'] = df['subject'].apply(lambda x: len(x))\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract subject features and add them to combined_df\n",
    "subject_features = extract_subject_features(combined_df)\n",
    "combined_df = pd.concat([combined_df.reset_index(drop=True), subject_features.reset_index(drop=True)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering for body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Updated function to count the number of spam keywords in the body\n",
    "def body_spam_count(body_content):\n",
    "    spam_keywords = [\n",
    "        \"account\", \"access\", \"bank\", \"credit\", \"click\", \"identity\", \"inconvenience\", \"information\", \"limited\",\n",
    "        \"log\", \"minutes\", \"password\", \"recently\", \"risk\", \"social\", \"security\", \"service\", \"suspended\", \"urgent\", \n",
    "        \"verify\", \"confirm\", \"locked\", \"unauthorized\", \"compromise\", \"refund\", \"billing\", \"update\", \"deactivate\", \n",
    "        \"subscription\", \"win\", \"prize\", \"offer\", \"discount\", \"reward\", \"free\", \"claim\", \"verify your account\", \n",
    "        \"reset password\", \"suspension\", \"fraud\", \"alert\", \"protection\", \"sign-in\", \"login\", \"activation\", \"urgent action\"\n",
    "    ]\n",
    "    return sum(1 for word in body_content.lower().split() if word in spam_keywords)\n",
    "\n",
    "\n",
    "# Feature engineering function\n",
    "def extract_body_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Basic text features\n",
    "    features['body_char_count'] = df['body'].apply(len)\n",
    "    features['body_word_count'] = df['body'].apply(lambda x: len(x.split()))\n",
    "    features['body_distinct_word_count'] = df['body'].apply(lambda x: len(set(x.split())))\n",
    "    features['body_average_word_length'] = features.apply(lambda row: row['body_char_count'] / row['body_word_count'] if row['body_word_count'] > 0 else 0, axis=1)\n",
    "\n",
    "    # Uppercase and numeric characters\n",
    "    features['body_uppercase_word_count'] = df['body'].apply(lambda x: sum(1 for word in x.split() if word.isupper()))\n",
    "    features['body_numeric_char_count'] = df['body'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "\n",
    "    # Punctuation features\n",
    "    features['body_exclamation_count'] = df['body'].apply(lambda x: x.count('!'))\n",
    "    features['body_question_count'] = df['body'].apply(lambda x: x.count('?'))\n",
    "    features['body_special_char_count'] = df['body'].apply(lambda x: len(re.findall(r'[#$%&@]', x)))\n",
    "\n",
    "    # Sentiment analysis\n",
    "    features['body_sentiment_polarity'] = df['body'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "    features['body_sentiment_subjectivity'] = df['body'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "    # URL analysis\n",
    "    features['body_url_count'] = df['body'].apply(lambda x: len(re.findall(r'http[s]?://', x)))\n",
    "    features['body_shortened_url_count'] = df['body'].apply(lambda x: len(re.findall(r'bit\\.ly|t\\.co|tinyurl', x)))\n",
    "\n",
    "    # Spam count check\n",
    "    features['body_spam_count'] = df['body'].apply(body_spam_count)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Generate features\n",
    "phishing_body_features = extract_body_features(combined_df)\n",
    "combined_df = pd.concat([combined_df.reset_index(drop=True), phishing_body_features.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering for sender (Uses receiver too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_email_domain(email):\n",
    "    \"\"\"Helper function to extract the domain from an email address.\"\"\"\n",
    "    match = re.search(r'@([\\w.-]+)', email)\n",
    "    return match.group(1).lower() if match else None  # Normalize to lowercase\n",
    "\n",
    "def send_diffSenderReceiver(send_address, receiver_addresses):\n",
    "    \"\"\"Check if sender domain is different from all receiver domains.\"\"\"\n",
    "    send_domain = get_email_domain(send_address)\n",
    "\n",
    "    # Split receiver addresses and get their domains\n",
    "    receiver_domains = [\n",
    "        get_email_domain(receiver) for receiver in receiver_addresses.split(',')\n",
    "    ]\n",
    "    \n",
    "    # Remove None values and normalize to lowercase\n",
    "    receiver_domains = [domain.lower() for domain in receiver_domains if domain]\n",
    "\n",
    "    # Check if any receiver domain is different from sender domain\n",
    "    if send_domain and receiver_domains:\n",
    "        return not all(domain == send_domain for domain in receiver_domains)  # Return True if any domain is different\n",
    "\n",
    "    return False  # If sender or receiver domains are invalid, return False\n",
    "\n",
    "# Functions to extract sender address based attributes\n",
    "def extract_sender_features(df):\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    # Integer: number of words in sender address\n",
    "    features['send_noWords'] = df['sender'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Integer: number of characters in sender address\n",
    "    features['send_noCharacters'] = df['sender'].apply(lambda x: len(x))\n",
    "\n",
    "    # Boolean: check if sender domain is different from all receiver domains\n",
    "    features['send_diffSenderReceiver'] = df.apply(\n",
    "        lambda row: send_diffSenderReceiver(row['sender'], row['receiver']), axis=1\n",
    "    )\n",
    "\n",
    "    return features\n",
    "\n",
    "# Extract features and add to combined_df\n",
    "features = extract_sender_features(combined_df)\n",
    "combined_df = pd.concat([combined_df.reset_index(drop=True), features.reset_index(drop=True)], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
